# CogVideoX diffusers Fine-tuning Guide

## Project Updates

ğŸš€ **We are excited to release the LoRA fine-tuning code for CogVideoX 1.5 by Diffusers**, designed specifically for image-to-video (image2video) tasks! This update brings significant improvements and new features to elevate your training experience. The full training startup code can be found in the `finetune` folder. Here's what's new:

### ğŸ”¥ Key Features & Improvements:
1. **ğŸ’¥ Bucket-based Multi-Resolution Training**: Unlock unparalleled model adaptability and performance across videos of all resolutions. This groundbreaking feature boosts the modelâ€™s ability to handle diverse video qualities with ease!
   
2. **âš¡ Fixed RoPE (Relative Position Encoding) Configuration Error**: Weâ€™ve optimized the position encoding mechanism, solving the error in the original CogVideo code, resulting in smoother training and higher-quality outputs. No more misconfigurationsâ€”just pure efficiency!

3. **ğŸ”§ Corrected OFS Embedding Issue**: Previously, OFS embedding was incorrectly set to None in the original code. Now, itâ€™s properly configured for stability and precision, improving the overall reliability and robustness of the model.

### âœ… Summary of Fixes:
- **Multi-resolution Support** ğŸ–¼ï¸
- **Accurate Positional Encoding** ğŸ“
- **Correct OFS Embedding Setup** ğŸ”‘
- **Optimized Multi-GPU Fine-Tuning** ğŸ’»

With these changes, you can now start training with **just one command**â€”simple, fast, and effective! 




ğŸš€ **æˆ‘ä»¬æ¿€åŠ¨åœ°å®£å¸ƒå‘å¸ƒäº†åŸºäº Diffusers çš„ LoRA å¾®è°ƒä»£ç ï¼Œç”¨äº CogVideoX 1.5ï¼Œå¹¶æ”¯æŒå›¾åƒåˆ°è§†é¢‘ï¼ˆimage2videoï¼‰ä»»åŠ¡ï¼** æœ¬æ¬¡æ›´æ–°å¸¦æ¥äº†é‡å¤§çš„æ”¹è¿›å’Œæ–°ç‰¹æ€§ï¼Œè®­ç»ƒå¯åŠ¨ä»£ç å¯ä»¥åœ¨ `finetune` æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ã€‚ä»¥ä¸‹æ˜¯ä¸»è¦çš„æ–°åŠŸèƒ½ï¼š

### ğŸ”¥ **å…³é”®åŠŸèƒ½ä¸æ”¹è¿›ï¼š**
1. **ğŸ’¥ åŸºäºæ¡¶çš„å¤šåˆ†è¾¨ç‡è®­ç»ƒ**ï¼šé‡Šæ”¾å‰æ‰€æœªæœ‰çš„æ¨¡å‹é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œé€‚ç”¨äºå„ç§åˆ†è¾¨ç‡çš„è§†é¢‘ã€‚æ­¤é¡¹çªç ´æ€§åŠŸèƒ½å¢å¼ºäº†æ¨¡å‹å¯¹ä¸åŒè§†é¢‘è´¨é‡çš„å¤„ç†èƒ½åŠ›ï¼
   
2. **âš¡ ä¿®å¤äº† RoPEï¼ˆç›¸å¯¹ä½ç½®ç¼–ç ï¼‰é…ç½®é”™è¯¯**ï¼šæˆ‘ä»¬ä¼˜åŒ–äº†ä½ç½®ç¼–ç æœºåˆ¶ï¼Œè§£å†³äº†åŸå§‹ CogVideo ä»£ç ä¸­çš„é”™è¯¯ï¼Œä»è€Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œè¾“å‡ºè´¨é‡ã€‚ä¸å†æœ‰é…ç½®é”™è¯¯â€”â€”åªå‰©ä¸‹é«˜æ•ˆè®­ç»ƒï¼

3. **ğŸ”§ è§£å†³äº† OFS åµŒå…¥è®¾ç½®ä¸º None çš„é—®é¢˜**ï¼šåŸå§‹ä»£ç ä¸­ï¼ŒOFS åµŒå…¥é”™è¯¯åœ°è®¾ç½®ä¸º Noneã€‚ç°åœ¨å®ƒå·²ç»æ­£ç¡®é…ç½®ï¼Œæå‡äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œç¡®ä¿äº†æ¨¡å‹çš„ç²¾å‡†æ€§ã€‚

### âœ… **ä¿®å¤æ±‡æ€»ï¼š**
- **å¤šåˆ†è¾¨ç‡æ”¯æŒ** ğŸ–¼ï¸
- **å‡†ç¡®çš„ä½ç½®ç¼–ç ** ğŸ“
- **æ­£ç¡®çš„ OFS åµŒå…¥è®¾ç½®** ğŸ”‘
- **ä¼˜åŒ–çš„å¤šå¡å¾®è°ƒ** ğŸ’»

é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œæ‚¨ç°åœ¨åªéœ€**ä¸€æ¡å‘½ä»¤**å°±èƒ½å¼€å§‹è®­ç»ƒâ€”â€”ç®€å•ã€å¿«é€Ÿã€æœ‰æ•ˆï¼



## Hardware Requirements

+ CogVideoX-5B-I2V 1 * A100

## Install Dependencies

Since the related code has not been merged into the diffusers release, you need to base your fine-tuning on the
diffusers branch. Please follow the steps below to install dependencies:

```shell
git clone https://github.com/huggingface/diffusers.git
cd diffusers # Now in Main branch
pip install -e .
```

## data preprocessing


We fellow [CogVideoX-Fun](https://github.com/aigc-apps/CogVideoX-Fun) data preprocessing method, They have provided a simple demo of training the Lora model through image data, which can be found in the [wiki](https://github.com/aigc-apps/CogVideoX-Fun/wiki/Training-Lora) for details.

A complete data preprocessing link for long video segmentation, cleaning, and description can refer to [README](cogvideox/video_caption/README.md) in the video captions section. 

If you want to train a text to image and video generation model. You need to arrange the dataset in this format.

```
ğŸ“¦ project/
â”œâ”€â”€ ğŸ“‚ datasets/
â”‚   â”œâ”€â”€ ğŸ“‚ internal_datasets/
â”‚       â”œâ”€â”€ ğŸ“‚ train/
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 00000001.mp4
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 00000002.jpg
â”‚       â”‚   â””â”€â”€ ğŸ“„ .....
â”‚       â””â”€â”€ ğŸ“„ json_of_internal_datasets.json
```

The json_of_internal_datasets.json is a standard JSON file. The file_path in the json can to be set as relative path, as shown in below:
```json
[
    {
      "file_path": "train/00000001.mp4",
      "text": "A group of young men in suits and sunglasses are walking down a city street.",
      "type": "video"
    },
    {
      "file_path": "train/00000002.jpg",
      "text": "A group of young men in suits and sunglasses are walking down a city street.",
      "type": "image"
    },
    .....
]
```

You can also set the path as absolute path as follow:
```json
[
    {
      "file_path": "/mnt/data/videos/00000001.mp4",
      "text": "A group of young men in suits and sunglasses are walking down a city street.",
      "type": "video"
    },
    {
      "file_path": "/mnt/data/train/00000001.jpg",
      "text": "A group of young men in suits and sunglasses are walking down a city street.",
      "type": "image"
    },
    .....
]
```

## Video DiT training
 
If the data format is relative path during data preprocessing, please set ```scripts/train.sh``` as follow.
```
export DATASET_NAME="datasets/internal_datasets/"
export DATASET_META_NAME="datasets/internal_datasets/json_of_internal_datasets.json"
```

If the data format is absolute path during data preprocessing, please set ```scripts/train.sh``` as follow.
```
export DATASET_NAME=""
export DATASET_META_NAME="/mnt/data/json_of_internal_datasets.json"
```

Then, we run scripts/train.sh.
```sh
sh scripts/train_cogvideox_i2v_lora_single_rank.sh
```

We can choose whether to use deep speed in CogVideoX-Fun, which can save a lot of video memory. 

Some parameters in the sh file can be confusing, and they are explained in this document:

- `enable_bucket` is used to enable bucket training. When enabled, the model does not crop the images and videos at the center, but instead, it trains the entire images and videos after grouping them into buckets based on resolution.
- `random_frame_crop` is used for random cropping on video frames to simulate videos with different frame counts.
- `random_hw_adapt` is used to enable automatic height and width scaling for images and videos. When random_hw_adapt is enabled, the training images will have their height and width set to image_sample_size as the maximum and video_sample_size as the minimum. For training videos, the height and width will be set to video_sample_size as the maximum and min(video_sample_size, 512) as the minimum.
- `training_with_video_token_length` specifies training the model according to token length. The token length for a video with dimensions 512x512 and 49 frames is 13,312.
  - At 512x512 resolution, the number of video frames is 49;
  - At 768x768 resolution, the number of video frames is 21;
  - At 1024x1024 resolution, the number of video frames is 9;
  - These resolutions combined with their corresponding lengths allow the model to generate videos of different sizes.

CogVideoX-5B-I2V-v1.5 without deepspeed:

```sh
export MODEL_PATH="./models/CogVideoX1.5-5B-I2V/"
export CACHE_PATH="./cache/"
export OUTPUT_PATH="output_dir/lora-v1"
export DATASET_META_NAME="./dataset/motion-dataset/train.json"
export DATASET_NAME=""

export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export NCCL_BLOCKING_WAIT=1
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_TIMEOUT_MS=1000000  # è®¾ç½®ä¸º 10minï¼Œæˆ–æ ¹æ®éœ€è¦å¢åŠ 


accelerate launch train_cogvideox_image_to_video_lora.py \
  --pretrained_model_name_or_path $MODEL_PATH \
  --cache_dir $CACHE_PATH \
  --train_data_dir=$DATASET_NAME \
  --train_data_meta=$DATASET_META_NAME \
  --image_sample_size=1280 \
  --video_sample_size=512 \
  --token_sample_size=512 \
  --random_hw_adapt \
  --video_sample_stride=3 \
  --video_sample_n_frames=49 \
  --video_repeat 1 \
  --mixed_precision bf16 \
  --validation_epochs 100 \
  --seed 42 \
  --rank 128 \
  --lora_alpha 64 \
  --output_dir $OUTPUT_PATH \
  --train_batch_size 1 \
  --num_train_epochs 30 \
  --checkpoint_epochs 1 \
  --gradient_checkpointing \
  --gradient_accumulation_steps 1 \
  --learning_rate 1e-4 \
  --lr_scheduler constant_with_warmup \
  --lr_warmup_steps 500 \
  --lr_num_cycles 1 \
  --optimizer AdamW \
  --adam_beta1 0.9 \
  --adam_beta2 0.95 \
  --max_grad_norm 1.0 \
  --allow_tf32 \
  --training_with_video_token_length \
  --enable_bucket \
  --lora_dropout 0.0 \
  --nccl_timeout $NCCL_TIMEOUT_MS

```


CogVideoX-5B-I2V-v1.5 with deepspeed:
```sh
export MODEL_NAME="models/Diffusion_Transformer/CogVideoX-Fun-2b-InP"
export DATASET_NAME="datasets/internal_datasets/"
export DATASET_META_NAME="datasets/internal_datasets/metadata.json"
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
NCCL_DEBUG=INFO

# When train model with multi machines, use "--config_file accelerate.yaml" instead of "--mixed_precision='bf16'".
accelerate launch --use_deepspeed --deepspeed_config_file scripts/zero_stage2_config.json --deepspeed_multinode_launcher standard scripts/train_lora.py \
   --pretrained_model_name_or_path $MODEL_PATH \
  --cache_dir $CACHE_PATH \
  --train_data_dir=$DATASET_NAME \
  --train_data_meta=$DATASET_META_NAME \
  --image_sample_size=1280 \
  --video_sample_size=512 \
  --token_sample_size=512 \
  --video_sample_stride=3 \
  --random_hw_adapt \
  --video_sample_n_frames=49 \
  --video_repeat 1 \
  --mixed_precision bf16 \
  --validation_epochs 100 \
  --seed 42 \
  --rank 128 \
  --lora_alpha 64 \
  --output_dir $OUTPUT_PATH \
  --train_batch_size 1 \
  --num_train_epochs 30 \
  --checkpoint_epochs 1 \
  --gradient_checkpointing \
  --gradient_accumulation_steps 1 \
  --learning_rate 1e-4 \
  --lr_scheduler constant_with_warmup \
  --lr_warmup_steps 500 \
  --lr_num_cycles 1 \
  --optimizer AdamW \
  --adam_beta1 0.9 \
  --adam_beta2 0.95 \
  --max_grad_norm 1.0 \
  --allow_tf32 \
  --training_with_video_token_length \
  --enable_bucket \
  --lora_dropout 0.0 \
  --nccl_timeout $NCCL_TIMEOUT_MS
```
